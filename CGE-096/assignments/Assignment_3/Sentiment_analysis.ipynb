{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment_analysis_using_RNN/MDS201803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "\n",
    "# Only used for data preprocessing\n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer \n",
    "   \n",
    "# Only used for generating summary stats\n",
    "from scipy import stats\n",
    "\n",
    "# Only used for keep tracking on the progress bar\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/subhasish/anaconda3/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/subhasish/anaconda3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining & Merging all the training reviews and test reviews in single files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for line in open('aclImdb/movie_data/full_train.txt', 'r'):\n",
    "    train_data.append(line.strip())\n",
    "    \n",
    "test_data = []\n",
    "for line in open('aclImdb/movie_data/full_test.txt', 'r'):\n",
    "    test_data.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For labels 1 : positive & 0 : negative\n",
    "\n",
    "train_labels = np.concatenate((np.ones(12500),np.zeros(12500))).astype(int)\n",
    "test_labels = np.concatenate((np.ones(12500),np.zeros(12500))).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the raw corpus, it is observed that the data contains punctuations, symbols etc. Since we are intereted in the english words only we preprocess the data to remove them.\n",
    "\n",
    "Regular Expressions are used for preprocessing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = re.compile(\"[!#$%&*.;:!\\'?,\\\"()\\[\\]]\")\n",
    "symbols = re.compile(\"(\\-)|(\\/)|(<br\\s*/><br\\s*/>)\")                          \n",
    "\n",
    "# The following function removes all punctuations & symbols/syntax from the data\n",
    "# also makes the data lower case\n",
    "\n",
    "def preprocess(data):\n",
    "    \n",
    "    output = []\n",
    "    for reviews in data:\n",
    "        reviews = punctuations.sub(\"\", reviews)\n",
    "        reviews = symbols.sub(\"\", reviews)\n",
    "        reviews = reviews.lower()\n",
    "        output.append(reviews)\n",
    "        \n",
    "    return output\n",
    "\n",
    "processed_train = preprocess(train_data)\n",
    "processed_test = preprocess(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words having too high or too low frequencies are of no use in our study. Most commonly occuring words like \"the\", \"a\" etc are present in all the reviews and contribute nothing in our analysis. Similarly there are some rare words also which have no discriminating power. We remove all such words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in our corpus :  11313560\n"
     ]
    }
   ],
   "source": [
    "combined_text_train = ' '.join(processed_train)\n",
    "combined_text_test = ' '.join(processed_test)\n",
    "combined_text = combined_text_train + combined_text_test\n",
    "\n",
    "word_list = combined_text.split()\n",
    "\n",
    "print(\"Total number of words in our corpus : \",len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_new = []\n",
    "for i in word_list:\n",
    "    if i not in stop_words:\n",
    "        word_list_new.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "word_list_lem = []\n",
    "for word in word_list_new:\n",
    "    word_list_lem.append(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So we are able to reduce our vocab size by 11169 using lemmatization\n"
     ]
    }
   ],
   "source": [
    "print(\"So we are able to reduce our vocab size by \" + str(len(set(word_list_new))-len(set(word_list_lem))) + \" using lemmatization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer() \n",
    "  \n",
    "word_list_stem = []\n",
    "for word in word_list_lem:\n",
    "    word_list_stem.append(ps.stem(word))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So we are able to reduce our vocab size by 29027 using stemming\n"
     ]
    }
   ],
   "source": [
    "print(\"So we are able to reduce our vocab size by \" + str(len(set(word_list_lem))-len(set(word_list_stem))) + \" using stemming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the unique words and sorting them w.r.t their counts\n",
    "\n",
    "# Dictionary of the unique words along with their counts\n",
    "vocab_dict = {}\n",
    "\n",
    "for i in word_list_stem:\n",
    "    try:\n",
    "        vocab_dict[i] += 1\n",
    "    except KeyError:\n",
    "        vocab_dict[i] = 1\n",
    "\n",
    "# Sorting the vocab_dict w.r.t their counts        \n",
    "vocab_dict_sorted = {k: v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we remove the words with extreme low frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=184069, minmax=(1, 98933), mean=32.56473387697005, variance=285532.9681308725, skewness=86.03721368463076, kurtosis=12502.3455189253)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_list = []\n",
    "for i in enumerate(vocab_dict_sorted):\n",
    "    freq_list.append(vocab_dict_sorted[i[1]])\n",
    "    \n",
    "# summary statistics for the frequency distributions of the words    \n",
    "stats.describe(np.array(freq_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9205986895907649"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(freq_list[:10000])/np.sum(freq_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see that the top $10k$ most frequent words account for $92\\%$ of the total frequency. Hence in order to reduce computation time and increase efficienc we restrict our vocabulary to $10k$ words only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict_new = {}\n",
    "count = 0\n",
    "for i in vocab_dict_sorted.keys():\n",
    "    vocab_dict_new[i] = vocab_dict_sorted[i]\n",
    "    count += 1\n",
    "    if count >= 9999:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words in our corpus :  9999\n"
     ]
    }
   ],
   "source": [
    "# Mapping each unique word to integers starting from 1\n",
    "vocab_int_mapping = {c:w+1 for (w,c) in enumerate(vocab_dict_new)}\n",
    "\n",
    "print(\"Total number of unique words in our corpus : \",len(vocab_int_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding each word in the reviews by their corresponding token numbers/indices\n",
    "# encoded_reviews is a list of list, where each sub-list contains the reviews which are encoded\n",
    "\n",
    "def encode(data):\n",
    "    \n",
    "    encoded_reviews = []\n",
    "\n",
    "    for review in data:\n",
    "\n",
    "        r = []\n",
    "        for word in review.split():\n",
    "            try:\n",
    "                r.append(vocab_int_mapping[word])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        encoded_reviews.append(r)\n",
    "    \n",
    "    return encoded_reviews\n",
    "    \n",
    "encoded_reviews_train = encode(processed_train)\n",
    "encoded_reviews_test = encode(processed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to input the review vectors in a RNN we need all of them of the same size, i.e. this sequence length is same as number of time steps for the RNN. To deal with both short and long reviews, we will pad the small ones and  truncate all the large reviews to a specific length. We denote this length by **review_length** .<br>\n",
    "For reviews shorter than *review_length*, we will pad with 0s. For reviews longer than *review_length* we will truncate them to the first *review_length* words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=25000, minmax=(1, 958), mean=80.39876, variance=3695.447808374735, skewness=2.2009391036269865, kurtosis=7.491137450034261)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_review_len = []\n",
    "for i in encoded_reviews_train:\n",
    "    train_review_len.append(len(i))\n",
    "\n",
    "stats.describe(np.array(train_review_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we take *review_length* = 80.<br>\n",
    "modifying all the reviews w.r.t the review_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_length = 200\n",
    "\n",
    "# This function truncates or pads the reviews as necessary based on their lengths\n",
    "def resize(data):\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for i in data:\n",
    "\n",
    "        if len(i)>review_length:\n",
    "            features.append(i[:review_length])\n",
    "        else:\n",
    "            features.append([0]*(review_length-len(i)) + i)\n",
    "    return features\n",
    "\n",
    "\n",
    "features_train = resize(encoded_reviews_train)\n",
    "features_test = resize(encoded_reviews_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have converted te reviews into list of integers which denotes the index of the pericular word in the vocabulary. Now to input a review into the Neural Network we need to encode each word using One Hot Encoding. Since we have $10000$ unique words in our vocabulary , each word in the review will be represented as a vector of size $10000 \\times 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(review):\n",
    "    \n",
    "    vocab_size = 10000\n",
    "    inputs = []\n",
    "    \n",
    "    for w in review:\n",
    "       \n",
    "        v = np.zeros((vocab_size, 1))\n",
    "        v[w] = 1\n",
    "        inputs.append(v)\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size=10):\n",
    "        \n",
    "        # Initializaing the weights randomly from uniform distribution\n",
    "        self.Wh = np.random.uniform(-np.sqrt(1./input_size), np.sqrt(1./input_size), (hidden_size, hidden_size))\n",
    "        self.Wx = np.random.uniform(-np.sqrt(1./input_size), np.sqrt(1./input_size), (hidden_size, input_size))\n",
    "        self.Wy = np.random.uniform(-np.sqrt(1./input_size), np.sqrt(1./input_size), (output_size, hidden_size))\n",
    "\n",
    "        # Initializing the biases to zero np arrays\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Here the function takes a singe review\n",
    "        # input is a list of length 116 & vocab_size is 50,000\n",
    "        # each element of the input list is one_hot_vector of size (vocab_size, 1)\n",
    "        \n",
    "        # initializing the Zero-th hidden layer (one corresponding to 0-th time stamp)\n",
    "        h = np.zeros((self.Wh.shape[0], 1))\n",
    "        \n",
    "        # Dictionary for storing the hidden layer units\n",
    "        # keys : time index ; values : hidden layer vector\n",
    "        self.hidden_layer = {}\n",
    "        \n",
    "        # storing the inputs and the initial hidden layer for future use\n",
    "        self.last_inputs = inputs\n",
    "        self.hidden_layer[0] = h \n",
    "        \n",
    "        # Performing the forward pass for each time step \n",
    "        for i, j in enumerate(inputs):\n",
    "\n",
    "            h = np.tanh(np.matmul(self.Wx, j) + np.matmul(self.Wh, h) + self.bh)\n",
    "            self.hidden_layer[i + 1] = h\n",
    "\n",
    "        # Final computation of the output using the last hidden layer\n",
    "        output = np.matmul(self.Wy, h) + self.by\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \n",
    "        # Softmax Function for the output array.\n",
    "        # Outputs 2-class probabilities which sums up to 1\n",
    "        return np.exp(x) / sum(np.exp(x))\n",
    "    \n",
    "    \n",
    "    def BPTT(self, d_y, learn_rate=0.1):\n",
    "        \n",
    "        # This function performs backpropagation through time (BPTT)\n",
    "        # We denote, (dL/dy) : d_y  it has shape (output_size, 1).\n",
    "        # dL/dy = p_i     , if i != c\n",
    "        #         p_i - 1 , if i == c\n",
    "        \n",
    "\n",
    "        # Derivative arrays initialized to zero \n",
    "        d_Wh = np.zeros(self.Wh.shape)\n",
    "        d_Wx = np.zeros(self.Wx.shape)\n",
    "        d_bh = np.zeros(self.bh.shape)\n",
    "        \n",
    "        # Derivatives dL/dWy and dL/dby.\n",
    "        d_Wy = np.matmul(d_y, self.hidden_layer[len(self.last_inputs)].T)\n",
    "        d_by = d_y\n",
    "\n",
    "        # Calculate dL/dh for the last hidden layer.\n",
    "        d_h = np.matmul(self.Wy.T, d_y)\n",
    "\n",
    "        # BPTT loop with the derivatives \n",
    "        for t in range(len(self.last_inputs))[::-1]:\n",
    "            \n",
    "            temp = ((1 - self.hidden_layer[t + 1] ** 2) * d_h)\n",
    "\n",
    "            d_bh += temp\n",
    "            d_Wh += np.matmul(temp, self.hidden_layer[t].T)\n",
    "            d_Wx += np.matmul(temp, self.last_inputs[t].T)\n",
    "            d_h = np.matmul(self.Wh, temp)  \n",
    "          \n",
    "        # Clip to prevent exploding gradients.\n",
    "        #for d in [d_Wx, d_Wh, d_Wy, d_bh, d_by]:\n",
    "        #    np.clip(d, -1, 1, out=d)\n",
    "\n",
    "        # Update weights and biases using gradient descent.\n",
    "        self.Wh -= learn_rate * d_Wh\n",
    "        self.Wx -= learn_rate * d_Wx\n",
    "        self.Wy -= learn_rate * d_Wy\n",
    "        self.bh -= learn_rate * d_bh\n",
    "        self.by -= learn_rate * d_by\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def processData(features, labels, backprop=True):\n",
    "\n",
    "    loss = 0\n",
    "    correct_pred = 0\n",
    "    \n",
    "    # Randomly picking up the training data\n",
    "    l = np.arange(len(features))\n",
    "    np.random.shuffle(l)\n",
    "    \n",
    "    with tqdm(total=len(features), file=sys.stdout) as pbar:\n",
    "        \n",
    "        for i in l:\n",
    "\n",
    "            inputs = one_hot_encoding(features[i])\n",
    "            target = labels[i]\n",
    "\n",
    "            # Forward pass\n",
    "            prediction = rnn.forward(inputs)\n",
    "            probs = rnn.softmax(prediction)\n",
    "\n",
    "            # Calculating loss & accuracy\n",
    "            loss -= np.log(probs[target])\n",
    "\n",
    "            if np.argmax(probs) == target:\n",
    "                correct_pred += 1\n",
    "\n",
    "            if backprop:\n",
    "                # Build dL/dy\n",
    "                d_L_d_y = probs\n",
    "\n",
    "                d_L_d_y[target] -= 1\n",
    "\n",
    "                # Backward\n",
    "                rnn.BPTT(d_L_d_y)\n",
    "            \n",
    "            # for updating the tqdm progress bar\n",
    "            pbar.update(1)\n",
    "            \n",
    "            \n",
    "    return loss / len(features), correct_pred / len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)\n",
    "\n",
    "# Initialize the RNN class\n",
    "rnn = RNN(10000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195eafb5375546b2807b91c588e9f13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1\n",
      "Train:\tLoss 0.693 | Accuracy: 0.534\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc4eef5a8f14d93a605157a8df0f5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test:\tLoss 0.708 | Accuracy: 0.544\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c323d89b3c304ff7929afb660293b545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 2\n",
      "Train:\tLoss 0.673 | Accuracy: 0.584\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf71fa70130c4d1fbb4dfa1ad5a4943c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test:\tLoss 0.691 | Accuracy: 0.556\n"
     ]
    }
   ],
   "source": [
    "# Training through epochs\n",
    "for epoch in range(2):\n",
    "    train_loss, train_acc = processData(features_train, train_labels, backprop=True)\n",
    " \n",
    "    print('--- Epoch %d' % (epoch + 1))\n",
    "    print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n",
    "\n",
    "    test_loss, test_acc = processData(features_test, test_labels, backprop=False)\n",
    "    print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99496"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1056589f004d48db9f05ff4bf93dec38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, acc = processData(features_train, train_labels, backprop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 4, 0])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99813265e-01]\n",
      " [1.86735251e-04]]\n"
     ]
    }
   ],
   "source": [
    "#inputs = np.array(features_train[1]).reshape(228,1)\n",
    "inputs = one_hot_encoding(features_train[900])\n",
    "out = rnn.forward(inputs)\n",
    "probs = rnn.softmax(out)\n",
    "print(probs) # [[0.50000095], [0.49999905]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(0,25000):\n",
    "    inputs = one_hot_encoding(features_train[i])\n",
    "    out = rnn.forward(inputs)\n",
    "    probs = rnn.softmax(out)\n",
    "    target = np.argmax(probs)\n",
    "    if target == 1:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gunzip -c aclImdb_v1.tar.gz | tar xopf -\n",
    "\n",
    "cd aclImdb\n",
    "\n",
    "mkdir movie_data\n",
    "\n",
    "# puts four files in the combined_files directory:\n",
    "# full_train.txt, full_test.txt, original_train_ratings.txt, and original_test_ratings.txt\n",
    "for split in train test;\n",
    "do\n",
    "\n",
    "  for sentiment in pos neg;\n",
    "  do \n",
    "    \n",
    "    for file in $split/$sentiment/*; \n",
    "    do\n",
    "              cat $file >> movie_data/full_${split}.txt; \n",
    "              echo >> movie_data/full_${split}.txt; \n",
    "\n",
    "\t     # This line adds files containing the original reviews if desired\n",
    "             # echo $file | cut -d '_' -f 2 | cut -d \".\" -f 1 >> combined_files/original_${split}_ratings.txt; \n",
    "    done;\n",
    "  done;\n",
    "done;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
