{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural_Language_Processing/Assignment_2/MDS201803"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start with a Simple Neural Network framework with only 1 hidden layers. Our Neural network has the following configuration :\n",
    " - Input Layer :\n",
    "   - Contains One-Hot vector representations of 5 letter English words  \n",
    "   - Number of nodes : 26\n",
    " - Hidden Layer :\n",
    "   - Only one hidden layer\n",
    "   - Number of nodes : 10\n",
    " - Output Layer :\n",
    "   - Contains the One hot vector representation of 128 words in our vocabulary\n",
    "   - Number of nodes : 128\n",
    "<br>   \n",
    "Our aim is to recognize the words, i.e. in given a $26 \\times 1$ input vector (representing a 5 letter English word) our Neural Network should output the One hot vector (a $128 \\times 1$ vector representing a single word in a vocabulary of 128 English word) corresponding to that word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [\"about\", \"above\",\"abuse\",\"added\",\"adult\",\"after\",\"again\",\"agent\",\"agree\",\n",
    "             \"beach\", \"began\", \"begin\", \"being\", \"below\", \"bible\", \"bills\", \"birds\", \"birth\",\n",
    "             \"calls\", \"cards\", \"carry\", \"cases\", \"catch\", \"cause\", \"cells\", \"chain\", \"chair\",\n",
    "             \"dates\", \"death\", \"depth\", \"doing\", \"doors\", \"doubt\", \"draft\", \"drawn\",\"dream\",\n",
    "             \"eight\", \"email\", \"empty\", \"ended\", \"enemy\", \"enjoy\", \"enter\", \"entry\", \"equal\",\n",
    "             \"facts\", \"faith\", \"feels\", \"fewer\", \"field\", \"fifth\", \"fight\", \"filed\", \"files\",\n",
    "             \"gifts\", \"girls\", \"given\", \"gives\", \"glass\", \"goals\", \"going\", \"goods\", \"grade\",\n",
    "             \"hands\", \"happy\", \"heads\", \"heard\", \"heart\", \"heavy\", \"hello\", \"helps\", \"hence\",\n",
    "             \"ideal\", \"ideas\", \"image\", \"index\", \"inner\", \"input\", \"issue\", \"items\", \"joint\",\n",
    "             \"judge\", \"juice\", \"keeps\", \"kinds\", \"knife\", \"known\", \"knows\", \"label\", \"labor\",\n",
    "             \"large\", \"later\", \"layer\", \"leads\", \"learn\", \"least\", \"leave\", \"legal\", \"level\",\n",
    "             \"magic\", \"major\", \"makes\", \"match\", \"maybe\", \"meals\", \"means\", \"meant\", \"media\",\n",
    "             \"never\", \"newly\", \"night\", \"noise\", \"north\", \"noted\", \"notes\", \"nurse\", \"occur\",\n",
    "             \"older\", \"opens\", \"order\", \"other\", \"owned\", \"owner\", \"pages\", \"paint\",\"panel\", \"paper\", \"parks\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating One-Hot vector encoding of the English words & Alphabets\n",
    "\n",
    "def one_hot_encode(w):\n",
    "    \n",
    "    output = np.zeros(26)\n",
    "    test_list = list(string.ascii_lowercase) \n",
    "    \n",
    "    for i in list(w):\n",
    "        index = test_list.index(i)\n",
    "        output[index] = output[index] + 1\n",
    "    \n",
    "    # takes the average of the one hot vectors of each alphabet of the word\n",
    "    output = output/5\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Input matrix of shape $128 \\times 26$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code stacks all the input One hot vectors and constructs the input matrix\n",
    "\n",
    "X = None\n",
    "for i in vocabulary:\n",
    "    x = one_hot_encode(i)\n",
    "    try:\n",
    "        X = np.row_stack([X, x])\n",
    "    except ValueError:\n",
    "        X = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the output matrix of shape $128 \\times 128$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.identity(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function learns parameters for the neural network and returns the model.\n",
    "\n",
    "#  iterations : Number of passes through the training data for gradient descent\n",
    "#  print_loss: If True, print the loss every 1000 iterations\n",
    "\n",
    "def Neural_network(hidden_dim, iterations):\n",
    "    \n",
    "    input_dim = 26           # input layer dimensionality\n",
    "    hidden_dim = 10          # hidden layer dimensionality\n",
    "    output_dim = 128         # output layer dimensionality\n",
    "\n",
    "    # parameters Gradient descent \n",
    "\n",
    "    eta = 0.01           # learning rate for gradient descent\n",
    "\n",
    "    # First we initialize the parameters to random values. \n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(input_dim, hidden_dim)    # First weight matrix of shape 26 x 10 \n",
    "    b1 = np.zeros((1, hidden_dim))                 # bias vector of shape 10 x 1\n",
    "    W2 = np.random.randn(hidden_dim, output_dim)   # Second weight matrix of shape 10 x 128\n",
    "    b2 = np.zeros((1, output_dim))                 # bias vector of shape 128 x 1\n",
    "\n",
    "    # We build the Neural Network model as a dictionary storing the parameters (Weights & Biases respectively)\n",
    "    param_dict = {}\n",
    "\n",
    "    # We use the Gradient descent algorithm to estimate the parameters by updating each batch\n",
    "    for i in range(0, iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        \n",
    "        z1 = X.dot(W1) + b1          # z1 obtained by multiplyng weight W1 and adding bias b1 with input\n",
    "        a1 = np.tanh(z1)             # a1 obtained after applying tanh() activation to z1\n",
    "        z2 = a1.dot(W2) + b2         # z2 obtained by multiplyng weight W2 and adding bias b2 with a1\n",
    "        exp_scores = np.exp(z2)           \n",
    "        sftmax = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # softmax scores to generate probabilities\n",
    "\n",
    "        # Backpropagation\n",
    "        \n",
    "        delta3 = sftmax - y            # taking difference of observed any predicted node values\n",
    "        dW2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "        dW1 = np.dot(X.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        \n",
    "        W1 = W1 - eta * dW1\n",
    "        b1 = b1 - eta * db1\n",
    "        W2 = W2 - eta * dW2\n",
    "        b2 = b2 - eta * db2\n",
    "\n",
    "        # Updating the model with new parameters\n",
    "        \n",
    "        param_dict = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "    return param_dict\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the output\n",
    "\n",
    "def predict(param_dict, x):\n",
    "    \n",
    "    # takes the weights and biases from the trained model\n",
    "    \n",
    "    W1, b1, W2, b2 = param_dict['W1'], param_dict['b1'], param_dict['W2'], param_dict['b2']\n",
    "    \n",
    "    # Forward propagation\n",
    "    \n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Neural_network(10, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the accuracy function we compute the accuracy of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the accuracy of the model\n",
    "# To check Training accuracy keep, train = True\n",
    "# To check Test accuracy keep, train = False, test = input dictionary\n",
    "# To print the false positive results keep the \"print_false\" flag on\n",
    "\n",
    "def accuracy(model, train = True, test = None, print_false = True):\n",
    "\n",
    "    if train:\n",
    "        count = 0\n",
    "        for i in vocabulary:\n",
    "            if predict(model, one_hot_encode(i))[0] == vocabulary.index(i):\n",
    "                count += 1\n",
    "            else:\n",
    "                if print_false:\n",
    "                    print(\"input word : %s, predicted word : %s, predicted index : %i, actual index : %i\" %(i, vocabulary[predict(model, one_hot_encode(i))[0]], predict(model, one_hot_encode(i))[0], vocabulary.index(i)))\n",
    "        return count/len(vocabulary)\n",
    "        \n",
    "    else:\n",
    "        count = 0\n",
    "        for i in list(test.keys()):\n",
    "            if vocabulary[predict(model, one_hot_encode(i))[0]] == test[i]:\n",
    "                count += 1\n",
    "                if print_false:\n",
    "                    print(\"input word : %s, predicted word : %s, expected word : %s, CORRECT PREDICTION\" %(i, vocabulary[predict(model, one_hot_encode(i))[0]], test[i]))\n",
    "            else:\n",
    "                if print_false:\n",
    "                    print(\"input word : %s, predicted word : %s, expected word : %s\" %(i, vocabulary[predict(model, one_hot_encode(i))[0]], test[i]))\n",
    "            \n",
    "        return count/len(list(test.keys()))\n",
    "    \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to plot the accuracy values corresponding the iteration values\n",
    "# This function finds the iteration size for which the two consecutive accuracies differ no more than 0.00001 such that\n",
    "# the accuracy is at least greater than some threshold or iteration number is less than some threshold\n",
    "\n",
    "def plot_accuracy(hdim , max_acc = 0.99, max_iter = 2500):\n",
    "    \n",
    "    acc_list = []\n",
    "    itr_list = []\n",
    "    iteration = 1\n",
    "    acc_1 = accuracy(Neural_network(hdim, iteration), train = True, print_false= False)\n",
    "    acc_2 = accuracy(Neural_network(hdim, iteration + 50), train = True, print_false= False)\n",
    "\n",
    "    while abs(acc_1-acc_2)>0.00001 or acc_1 < 0.99 and iteration < 2500:\n",
    "\n",
    "        iteration += 50\n",
    "        acc_1 = accuracy(Neural_network(hdim, iteration), train = True, print_false= False)\n",
    "        acc_2 = accuracy(Neural_network(hdim, iteration + 50), train = True, print_false= False)\n",
    "        acc_list.append(acc_1)\n",
    "        itr_list.append(iteration)\n",
    "        \n",
    "    print(\"accuracy achieved : \",acc_1)\n",
    "    print(\"best iteration :\", iteration) \n",
    "    \n",
    "    return acc_list, itr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy achieved :  0.984375\n",
      "best iteration : 2501\n"
     ]
    }
   ],
   "source": [
    "accuracy_list, iteration_list = plot_accuracy(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we see that the best accuracy achieved after 2500 iteration (i.e. 2500 batch gradiant descent updates) is $98.43 \\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9dfba97278>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting Training accuracy w.r.t iteration \n",
    "\n",
    "fig = plt.figure()\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"training accuracy\")\n",
    "plt.plot(iteration_list,accuracy_list, '-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see that the accuracy improves till $500$ iterations and almost stops improving after $700-1000$ iterations. This graph helps us to identify a suitable iteration value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word : being, predicted word : begin, predicted index : 11, actual index : 12\n",
      "input word : field, predicted word : filed, predicted index : 52, actual index : 49\n",
      "input word : ideas, predicted word : ended, predicted index : 39, actual index : 73\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9765625"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model, train = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the Neural Network with $10$ nodes in the hidden layer and for $500$ iterations. After some trial and parameter tuning we found that the training accuracy of the model is $97.65 \\%$. The words it misclassifies are as above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Testing with mis-spelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\"ajent\" : \"agent\",\n",
    "             \"abouu\" : \"about\", \n",
    "             \"indes\" : \"index\", \n",
    "             \"abain\" : \"again\", \n",
    "             \"layer\" : \"later\", \n",
    "             \"aappy\" : \"happy\",\n",
    "             \"march\" : \"match\",\n",
    "             \"majik\" : \"magic\",\n",
    "             \"layre\" : \"layer\",\n",
    "             \"gless\" : \"glass\",\n",
    "             \"billo\" : \"bills\",\n",
    "             \"other\" : \"othre\",\n",
    "             \"juict\" : \"juice\",\n",
    "             \"feelz\" : \"feels\",\n",
    "             \"knofe\" : \"knife\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word : ajent, predicted word : paint, expected word : agent\n",
      "input word : abouu, predicted word : about, expected word : about, CORRECT PREDICTION\n",
      "input word : indes, predicted word : field, expected word : index\n",
      "input word : abain, predicted word : again, expected word : again, CORRECT PREDICTION\n",
      "input word : layer, predicted word : layer, expected word : later\n",
      "input word : aappy, predicted word : happy, expected word : happy, CORRECT PREDICTION\n",
      "input word : march, predicted word : match, expected word : match, CORRECT PREDICTION\n",
      "input word : majik, predicted word : again, expected word : magic\n",
      "input word : layre, predicted word : layer, expected word : layer, CORRECT PREDICTION\n",
      "input word : gless, predicted word : girls, expected word : glass\n",
      "input word : billo, predicted word : bills, expected word : bills, CORRECT PREDICTION\n",
      "input word : other, predicted word : other, expected word : othre\n",
      "input word : juict, predicted word : juice, expected word : juice, CORRECT PREDICTION\n",
      "input word : feelz, predicted word : feels, expected word : feels, CORRECT PREDICTION\n",
      "input word : knofe, predicted word : knife, expected word : knife, CORRECT PREDICTION\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model, train = False, test = test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our trained model is able to predict the correct words for the mis-spelled inputs. We used a test dataset of size $15$ mis-spelled words. The test accuracy obtained was $60 \\%$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
